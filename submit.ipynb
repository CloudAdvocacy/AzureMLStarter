{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure ML Experiment Submission\n",
    "\n",
    "To use this notebook, you need to download `config.json` file from Azure ML Workspace and place it in this folder. This will allow us to get the workspace reference right away:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AzMLWorkspace\tnortheurope\tAzureMLGroup\tnortheurope\n",
      "Library configuration succeeded\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "    print(ws.name, ws.location, ws.resource_group, ws.location, sep='\\t')\n",
    "    print('Library configuration succeeded')\n",
    "except:\n",
    "    print('Workspace not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then make sure we have the compute cluster. If the cluster does not exist - we will create it programmatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cluster_name = \"AzMLCompute\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2',\n",
    "                                                           vm_priority='lowpriority',\n",
    "                                                           min_nodes=1,\n",
    "                                                           max_nodes=4)\n",
    "    cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now upload the MNIST dataset into the Azure ML Workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Uploading ./dataset\\mnist.pkl\n",
      "Uploaded ./dataset\\mnist.pkl, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_fd2c443573714e31ba4b8638d8e3a37d"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "ds.upload('./dataset', target_path='mnist_data', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create training script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mytrain.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mytrain.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from azureml.core import Run\n",
    "from azureml.core.model import Model\n",
    "import pickle\n",
    "import keras\n",
    "from keras.layers import Dense,Dropout\n",
    "\n",
    "parser = argparse.ArgumentParser(description='MNIST Train')\n",
    "parser.add_argument('--data_folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "parser.add_argument('--epochs', type=int, default=3)\n",
    "parser.add_argument('--batch_size', type=int, default=128)\n",
    "parser.add_argument('--dropout', type=float)\n",
    "parser.add_argument('--hidden', type=int, default=100)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "mnist_fn = os.path.join(args.data_folder, 'mnist_data','mnist.pkl')\n",
    "mnist_fn = 'dataset/mnist.pkl'\n",
    "with open(mnist_fn,'rb') as f:\n",
    "    X,y = pickle.load(f)\n",
    "\n",
    "X /= 255.0\n",
    "y = keras.utils.to_categorical(y,10)\n",
    "\n",
    "n = int(0.8*X.shape[0])\n",
    "x_train = X[0:n]\n",
    "y_train = y[0:n]\n",
    "x_test = X[n:]\n",
    "y_test = y[n:]\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(Dense(args.hidden,input_shape=(784,),activation='relu'))\n",
    "if args.dropout is not None and args.dropout<1:\n",
    "    model.add(Dropout(args.dropout))\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=args.batch_size,\n",
    "          epochs=args.epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "os.makedirs('outputs',exist_ok=True)\n",
    "model.save('outputs/mnist_model.hdf5')\n",
    "\n",
    "# Log metrics\n",
    "run = Run.get_context()\n",
    "run.log('Test Loss', score[0])\n",
    "run.log('Accuracy', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's submit the experiment to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "experiment_name = 'Keras-MNIST'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "script_params = {\n",
    "    '--data_folder': ws.get_default_datastore(),\n",
    "}\n",
    "\n",
    "est = Estimator(source_directory='.',\n",
    "                script_params=script_params,\n",
    "                compute_target=cluster,\n",
    "                entry_script='mytrain.py',\n",
    "                pip_packages=['keras','tensorflow']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization using Hyperdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import *\n",
    "\n",
    "param_sampling = RandomParameterSampling({\n",
    "         '--hidden': choice([50,100,200,300]),\n",
    "         '--batch_size': choice([64,128]), \n",
    "         '--epochs': choice([5,10,50]),\n",
    "         '--dropout': choice([0.5,0.8,1])\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_termination_policy = MedianStoppingPolicy(evaluation_interval=1, delay_evaluation=0)\n",
    "hd_config = HyperDriveConfig(estimator=est,\n",
    "                            hyperparameter_sampling=param_sampling,\n",
    "                            policy=early_termination_policy,\n",
    "                            primary_metric_name='Accuracy',\n",
    "                            primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                            max_total_runs=16,\n",
    "                            max_concurrent_runs=4)\n",
    "experiment = Experiment(workspace=ws, name='keras-hyperdrive')\n",
    "hyperdrive_run = experiment.submit(hd_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azureml.train.restclients.hyperdrive.models.cancel_experiment_respose_dto.CancelExperimentResposeDto at 0x157a55ff688>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperdrive_run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registrering the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(Experiment: keras-hyperdrive,\n",
      "Id: keras-hyperdrive_1575928171879731_15,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Completed)\n",
      "Best accuracy: 0.9702500104904175\n"
     ]
    }
   ],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "print(best_run)\n",
    "print('Best accuracy: {}'.format(best_run_metrics['Accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(workspace=Workspace.create(name='AzMLWorkspace', subscription_id='d04ba089-715a-45b1-b4a3-2ce0fd60316f', resource_group='AzureMLGroup'), name=mnist_keras, id=mnist_keras:1, version=1, tags={}, properties={})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run.register_model(model_name='mnist_keras', model_path='outputs/mnist_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
